# Crisis Management Web Scraper

A web scraper that monitors current events from multiple news sources, ranks them by severity, and displays them on a GitHub Pages website.

## Features

- ğŸ” **Multi-Source Scraping**: Fetches news from RSS feeds and APIs
- ğŸ“Š **Severity Ranking**: Automatically ranks events by crisis level
- ğŸŒ **Live Website**: Displays results on GitHub Pages
- âš¡ **Automated Updates**: GitHub Actions runs scraper automatically
- ğŸ“± **Responsive Design**: Mobile-friendly interface

## Severity Criteria

Events are ranked based on:
1. **Keywords**: disaster, crisis, emergency, fatal, casualties
2. **Geographic Scope**: local, regional, national, global
3. **Impact Level**: affected population, economic impact
4. **Urgency**: breaking news vs. ongoing situations

## Setup

### Prerequisites
- Python 3.9 or higher
- Git

### Installation

1. Clone the repository:
```bash
git clone https://github.com/YOUR_USERNAME/crisis-management-scraper.git
cd crisis-management-scraper
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure news sources in `config/sources.json`

4. Run the scraper:
```bash
python scraper/main.py
```

### Live Prices Local Proxy (testing)
For local testing without CORS issues, start the lightweight proxy server:

```bash
python3 scraper/proxy_server.py 8001
```

The frontend auto-detects `localhost` and routes Yahoo Finance requests through `http://localhost:8001/proxy?url=...`.

When not on localhost, it falls back to public proxies (Jina/AllOrigins). We can later switch these calls to a free, documented market API for production.

### Run static site and proxy together
Use the helper script to launch both servers (serves repo root so `/data/events.json` is available):

```bash
bash scripts/run_local.sh
```

Ports are configurable via environment variables:

```bash
PROXY_PORT=8002 STATIC_PORT=8010 bash scripts/run_local.sh

Open the site at `http://localhost:STATIC_PORT/docs/` (e.g., `http://localhost:8010/docs/`).

Alternatively, just open the root and it will redirect:

```
http://localhost:8010/
```
```

## GitHub Pages Deployment

1. Enable GitHub Pages in repository settings
2. Set source to the `docs/` folder
3. GitHub Actions will automatically update the site

## Configuration

Edit `config/sources.json` to add or remove news sources:

```json
{
  "sources": [
    {
      "name": "Example News",
      "type": "rss",
      "url": "https://example.com/rss",
      "enabled": true
    }
  ]
}
```

Edit `config/severity_rules.json` to customize ranking criteria.

## Project Structure

```
.
â”œâ”€â”€ scraper/              # Python scraping modules
â”‚   â”œâ”€â”€ main.py          # Main scraper script
â”‚   â”œâ”€â”€ fetcher.py       # News fetching logic
â”‚   â””â”€â”€ ranker.py        # Severity ranking algorithm
â”œâ”€â”€ docs/                # GitHub Pages website
â”‚   â”œâ”€â”€ index.html       # Main webpage
â”‚   â”œâ”€â”€ styles.css       # Styling
â”‚   â””â”€â”€ app.js           # Frontend logic
â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ sources.json     # News sources
â”‚   â””â”€â”€ severity_rules.json  # Ranking criteria
â”œâ”€â”€ data/                # Generated data files
â”‚   â””â”€â”€ events.json      # Latest events data
â””â”€â”€ .github/workflows/   # GitHub Actions
    â””â”€â”€ scrape.yml       # Automated scraping workflow

```

## License

MIT License

## Contributing

Pull requests are welcome! Please ensure code follows PEP 8 standards.
